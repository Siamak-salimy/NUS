 الگوریتم درخت تصمیم (Decision Tree Algorithm)

الگوریتم درخت تصمیم یکی از الگوریتم‌های یادگیری ماشین است که برای حل مسائل طبقه‌بندی (Classification) و رگرسیون (Regression) استفاده می‌شود. این الگوریتم ساختاری شبیه به یک درخت دارد که شامل گره‌ها (Nodes) و شاخه‌ها (Branches) است. هدف این الگوریتم، تقسیم داده‌ها به زیرمجموعه‌هایی است که در نهایت به پیش‌بینی یا طبقه‌بندی داده‌ها منجر می‌شود.
---

 اجزای اصلی درخت تصمیم
1. گره ریشه (Root Node):
   - اولین گره درخت که کل مجموعه داده را نمایش می‌دهد.
   - این گره براساس ویژگی‌های داده‌ها به زیرمجموعه‌های کوچکتر تقسیم می‌شود.

2. گره‌های داخلی (Internal Nodes):
   - گره‌هایی که بین گره ریشه و برگ‌ها قرار دارند.
   - هر گره داخلی براساس یک ویژگی خاص داده‌ها را به زیرمجموعه‌های جدید تقسیم می‌کند.

3. برگ‌ها (Leaf Nodes):
   - آخرین گره‌های درخت که نتیجه نهایی (طبقه‌بندی یا پیش‌بینی) را نشان می‌دهند.

4. شاخه‌ها (Branches):
   - خطوطی که گره‌ها را به یکدیگر متصل می‌کنند و نشان‌دهنده تصمیمات مختلف هستند.

---

 مراحل ساخت درخت تصمیم
1. انتخاب ویژگی مناسب:
   - برای تقسیم داده‌ها، بهترین ویژگی انتخاب می‌شود. معیارهای مختلفی برای انتخاب ویژگی وجود دارد:
     - Information Gain: میزان کاهش عدم قطعیت (Entropy) پس از تقسیم داده‌ها.
     - Gini Index: معیاری برای اندازه‌گیری خلوص گره‌ها.
     - Gain Ratio: نسبت Information Gain به تعداد زیرمجموعه‌ها.

2. تقسیم داده‌ها:
   - داده‌ها براساس مقادیر ویژگی انتخاب‌شده به زیرمجموعه‌هایی تقسیم می‌شوند.

3. ایجاد زیردرخت‌ها:
   - برای هر زیرمجموعه، مراحل قبلی تکرار می‌شود تا به برگ‌ها برسیم.

4. توقف درخت:
   - درخت زمانی متوقف می‌شود که:
     - تمام داده‌ها در یک گره به یک کلاس تعلق داشته باشند.
     - عمق درخت به حد مجاز برسد.
     - تعداد داده‌ها در یک گره کمتر از حد آستانه باشد.

---

 معیارهای تقسیم‌بندی
1. Entropy و Information Gain:
   - Entropy میزان عدم قطعیت در داده‌ها را نشان می‌دهد.
   - Information Gain میزان کاهش Entropy پس از تقسیم داده‌ها را محاسبه می‌کند:
     \[
     IG(T, a) = Entropy(T) - \sum_{v \in Values(a)} \frac{|T_v|}{|T|} Entropy(T_v)
     \]

2. Gini Index:
   - Gini Index میزان ناهمگونی در داده‌ها را اندازه‌گیری می‌کند:
     \[
     Gini(T) = 1 - \sum_{i=1}^{n} p_i^2
     \]
     که \(p_i\) احتمال تعلق داده‌ها به کلاس \(i\) است.

3. Gain Ratio:
   - Gain Ratio برای حل مشکل Skewness در Information Gain استفاده می‌شود:
     \[
     Gain\ Ratio = \frac{IG(T, a)}{Split\ Information(a)}
     \]

---

 مزایای درخت تصمیم
1. قابلیت تفسیر:
   - درخت تصمیم به صورت گرافیکی قابل تفسیر است و تصمیمات آن قابل فهم برای انسان است.

2. کاربرد گسترده:
   - برای مسائل طبقه‌بندی و رگرسیون قابل استفاده است.

3. تعامل با داده‌های نویزی:
   - درخت تصمیم می‌تواند با داده‌های نویزی و ناقص کار کند.

4. پشتیبانی از داده‌های عددی و دسته‌ای:
   - این الگوریتم می‌تواند هر دو نوع داده را مدیریت کند.

---

 معایب درخت تصمیم
1. پیچیدگی:
   - درخت‌های بزرگ ممکن است پیچیده و غیرقابل تفسیر باشند.

2. Overfitting:
   - درخت‌های عمیق ممکن است به داده‌های آموزشی Overfit شوند.

3. حساسیت به تغییرات داده:
   - تغییرات کوچک در داده‌ها می‌تواند منجر به تغییرات زیاد در ساختار درخت شود.

4. محاسبات سنگین:
   - برای داده‌های بزرگ، محاسبات ممکن است زمان‌بر باشد.

---

 مثال عملی
فرض کنید داده‌های زیر داریم:

| هوای بیرون | دما | رطوبت | وزش باد | فعالیت |
|------------|------|--------|----------|---------|
| آفتابی    | گرم  | بالا   | ضعیف    | نه      |
| آفتابی    | گرم  | بالا   | قوی      | نه      |
| ابری       | گرم  | بالا   | ضعیف    | بله     |
| بارانی     | معتدل| بالا   | ضعیف    | بله     |
| بارانی     | سرد  | پایین  | ضعیف    | بله     |
| بارانی     | سرد  | پایین  | قوی      | نه      |

برای ساخت درخت تصمیم:
1. ابتدا ویژگی‌ها را براساس Information Gain یا Gini Index ارزیابی می‌کنیم.
2. ویژگی با بیشترین Information Gain (مثلاً "هوای بیرون") به عنوان گره ریشه انتخاب می‌شود.
3. داده‌ها براساس مقادیر این ویژگی تقسیم می‌شوند.
4. برای هر زیرمجموعه، مراحل فوق تکرار می‌شود تا به برگ‌ها برسیم.

---

 کاربردهای درخت تصمیم
1. پزشکی:
   - تشخیص بیماری‌ها براساس علائم.
2. مالی:
   - پیش‌بینی ریسک اعتباری مشتریان.
3. بازاریابی:
   - طبقه‌بندی مشتریان براساس رفتار خرید.
4. آموزش:
   - پیش‌بینی موفقیت دانش‌آموزان.

---

 کتابخانه‌های پیاده‌سازی درخت تصمیم در پایتون
1. Scikit-learn:
   ```python
   from sklearn.tree import DecisionTreeClassifier
   clf = DecisionTreeClassifier()
   clf.fit(X_train, y_train)
   ```
2. Graphviz:
   برای تجسم درخت تصمیم:
   ```python
   from sklearn.tree import export_graphviz
   import graphviz
   dot_data = export_graphviz(clf, out_file=None, feature_names=features)
   graph = graphviz.Source(dot_data)
   graph.render("tree")
   ```

---

 جمع‌بندی
الگوریتم درخت تصمیم یکی از ساده‌ترین و قابل فهم‌ترین الگوریتم‌های یادگیری ماشین است که برای مسائل طبقه‌بندی و رگرسیون استفاده می‌شود. با این حال، باید به مشکلاتی مانند Overfitting و پیچیدگی درخت توجه کرد و از تکنیک‌هایی مانند Pruning استفاده کرد.
